{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Length_Analysis_CUDA_Numba.ipynb",
      "provenance": [],
      "accelerator": "GPU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  }, "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word Length Analysis with CUDA and Numba\n",
        "\n",
        "This notebook demonstrates GPU acceleration using Numba for a simple text analysis task.\n",
        "\n",
        "Key concepts covered:\n",
        "- Basic CUDA kernel implementation\n",
        "- Memory transfer between CPU and GPU\n",
        "- Atomic operations\n",
        "- Grid/block configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_cell"
      },
      "source": [
        "!pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda, vectorize\n",
        "import math\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CPU Implementation\n",
        "Basic sequential implementation for comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpu_impl"
      },
      "source": [
        "def count_word_lengths_cpu(words):\n",
        "    max_length = 20\n",
        "    counts = np.zeros(max_length, dtype=np.int32)\n",
        "    for word in words:\n",
        "        if len(word) < max_length:\n",
        "            counts[len(word)] += 1\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Implementations\n",
        "Two approaches: vectorized and CUDA kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpu_impl"
      },
      "source": [
        "@vectorize(['int32(int32)'], target='cuda')\n",
        "def get_word_length(length):\n",
        "    return min(length, 20)\n",
        "\n",
        "@cuda.jit\n",
        "def count_word_lengths_gpu(word_lengths, counts):\n",
        "    idx = cuda.grid(1)\n",
        "    if idx < word_lengths.size:\n",
        "        length = word_lengths[idx]\n",
        "        if length < 20:\n",
        "            cuda.atomic.add(counts, length, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Analysis Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "analysis_func"
      },
      "source": [
        "def analyze_text(text, use_gpu=True):\n",
        "    words = text.split()\n",
        "    word_lengths = np.array([len(word) for word in words], dtype=np.int32)\n",
        "    \n",
        "    if not use_gpu:\n",
        "        return count_word_lengths_cpu(words)\n",
        "    \n",
        "    d_lengths = cuda.to_device(word_lengths)\n",
        "    d_counts = cuda.to_device(np.zeros(20, dtype=np.int32))\n",
        "    \n",
        "    threadsperblock = 256\n",
        "    blockspergrid = (word_lengths.size + threadsperblock - 1) // threadsperblock\n",
        "    \n",
        "    count_word_lengths_gpu[blockspergrid, threadsperblock](d_lengths, d_counts)\n",
        "    \n",
        "    return d_counts.copy_to_host()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test and Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "benchmark"
      },
      "source": [
        "# Create test data\n",
        "words = [\"python\", \"cuda\", \"gpu\", \"computing\"] * 250000\n",
        "text = \" \".join(words)\n",
        "\n",
        "# CPU benchmark\n",
        "t0 = time()\n",
        "cpu_counts = analyze_text(text, use_gpu=False)\n",
        "cpu_time = time() - t0\n",
        "\n",
        "# GPU benchmark\n",
        "t0 = time()\n",
        "gpu_counts = analyze_text(text, use_gpu=True)\n",
        "gpu_time = time() - t0\n",
        "\n",
        "print(f\"CPU time: {cpu_time:.4f}s\")\n",
        "print(f\"GPU time: {gpu_time:.4f}s\")\n",
        "print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nWord length frequencies:\")\n",
        "for length, count in enumerate(gpu_counts):\n",
        "    if count > 0:\n",
        "        print(f\"Length {length}: {count}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word_Length_Analysis_CUDA_Numba.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  }
}

